diff -r 6ddd2bf5d55e -r b84a44392603 memory/mozjemalloc/mozjemalloc.cpp
--- a/memory/mozjemalloc/mozjemalloc.cpp	Thu May 25 14:05:34 2017 -0700
+++ b/memory/mozjemalloc/mozjemalloc.cpp	Thu Jul 06 14:56:07 2017 +0900
@@ -795,6 +795,13 @@
 	arena_bin_t		bins[1]; /* Dynamically sized. */
 };
 
+enum ChunkType {
+  UNKNOWN_CHUNK,
+  ARENA_CHUNK,    // used to back arena runs created by arena_run_alloc
+  HUGE_CHUNK,     // used to back huge allocations (e.g. huge_malloc)
+  RECYCLED_CHUNK, // chunk has been stored for future use by chunk_recycle
+};
+
 /******************************************************************************/
 /*
  * Data.
@@ -1027,7 +1034,7 @@
  */
 
 static void	*chunk_alloc(size_t size, size_t alignment, bool base, bool zero);
-static void	chunk_dealloc(void *chunk, size_t size);
+static void	chunk_dealloc(void *chunk, size_t size, enum ChunkType type);
 static arena_t	*arenas_extend();
 static void	*huge_malloc(size_t size, bool zero);
 static void	*huge_palloc(size_t size, size_t alignment, bool zero);
@@ -2021,7 +2028,7 @@
 			malloc_mutex_unlock(&chunks_mtx);
 			node = base_node_alloc();
 			if (!node) {
-				chunk_dealloc(ret, size);
+				chunk_dealloc(ret, size, RECYCLED_CHUNK);
 				return nullptr;
 			}
 			malloc_mutex_lock(&chunks_mtx);
@@ -2098,7 +2105,7 @@
 
 	if (ret && base == false) {
 		if (malloc_rtree_set(chunk_rtree, (uintptr_t)ret, ret)) {
-			chunk_dealloc(ret, size);
+			chunk_dealloc(ret, size, UNKNOWN_CHUNK);
 			return nullptr;
 		}
 	}
@@ -2109,13 +2116,19 @@
 
 static void
 chunk_record(extent_tree_t *chunks_szad, extent_tree_t *chunks_ad, void *chunk,
-    size_t size)
+    size_t size, enum ChunkType type)
 {
 	bool unzeroed;
 	extent_node_t *xnode, *node, *prev, *xprev, key;
 
 	unzeroed = pages_purge(chunk, size);
 
+	/* If purge doesn't zero the chunk, only record arena chunks or
+	 * previously recycled chunks. */
+	if (unzeroed && type != ARENA_CHUNK && type != RECYCLED_CHUNK) {
+		return;
+	}
+
 	/*
 	 * Allocate a node before acquiring chunks_mtx even though it might not
 	 * be needed, because base_node_alloc() may cause a new base chunk to
@@ -2209,7 +2222,7 @@
 #undef CAN_RECYCLE
 
 static void
-chunk_dealloc(void *chunk, size_t size)
+chunk_dealloc(void *chunk, size_t size, enum ChunkType type)
 {
 
 	MOZ_ASSERT(chunk);
@@ -2220,7 +2233,7 @@
 	malloc_rtree_set(chunk_rtree, (uintptr_t)chunk, nullptr);
 
 	if (chunk_dalloc_mmap(chunk, size))
-		chunk_record(&chunks_szad_mmap, &chunks_ad_mmap, chunk, size);
+		chunk_record(&chunks_szad_mmap, &chunks_ad_mmap, chunk, size, type);
 }
 
 /*
@@ -2704,7 +2717,7 @@
 		LinkedList_Remove(&arena->spare->chunks_madvised_elem);
 #endif
 
-		chunk_dealloc((void *)arena->spare, chunksize);
+		chunk_dealloc((void *)arena->spare, chunksize, ARENA_CHUNK);
 		arena->stats.mapped -= chunksize;
 		arena->stats.committed -= arena_chunk_header_npages;
 	}
@@ -4222,7 +4235,7 @@
 	malloc_mutex_unlock(&huge_mtx);
 
 	/* Unmap chunk. */
-	chunk_dealloc(node->addr, CHUNK_CEILING(node->size));
+	chunk_dealloc(node->addr, CHUNK_CEILING(node->size), HUGE_CHUNK);
 
 	base_node_dealloc(node);
 }
