static int priv_op_read_msr(unsigned int reg, uint64_t *val,
                            struct x86_emulate_ctxt *ctxt)
{
    struct priv_op_ctxt *poc = container_of(ctxt, struct priv_op_ctxt, ctxt);
    const struct vcpu *curr = current;
    const struct domain *currd = curr->domain;
    bool vpmu_msr = false;

    switch ( reg )
    {
        int rc;

    case MSR_FS_BASE:
        if ( is_pv_32bit_domain(currd) )
            break;
        *val = cpu_has_fsgsbase ? __rdfsbase() : curr->arch.pv_vcpu.fs_base;
        return X86EMUL_OKAY;

    case MSR_GS_BASE:
        if ( is_pv_32bit_domain(currd) )
            break;
        *val = cpu_has_fsgsbase ? __rdgsbase()
                                : curr->arch.pv_vcpu.gs_base_kernel;
        return X86EMUL_OKAY;

    case MSR_SHADOW_GS_BASE:
        if ( is_pv_32bit_domain(currd) )
            break;
        *val = curr->arch.pv_vcpu.gs_base_user;
        return X86EMUL_OKAY;

    /*
     * In order to fully retain original behavior, defer calling
     * pv_soft_rdtsc() until after emulation. This may want/need to be
     * reconsidered.
     */
    case MSR_IA32_TSC:
        poc->tsc |= TSC_BASE;
        goto normal;

    case MSR_TSC_AUX:
        poc->tsc |= TSC_AUX;
        if ( cpu_has_rdtscp )
            goto normal;
        *val = 0;
        return X86EMUL_OKAY;

    case MSR_EFER:
        /* Hide unknown bits, and unconditionally hide SVME from guests. */
        *val = read_efer() & EFER_KNOWN_MASK & ~EFER_SVME;
        /*
         * Hide the 64-bit features from 32-bit guests.  SCE has
         * vendor-dependent behaviour.
         */
        if ( is_pv_32bit_domain(currd) )
            *val &= ~(EFER_LME | EFER_LMA | EFER_LMSLE |
                      (boot_cpu_data.x86_vendor == X86_VENDOR_INTEL
                       ? EFER_SCE : 0));
        return X86EMUL_OKAY;

    case MSR_K7_FID_VID_CTL:
    case MSR_K7_FID_VID_STATUS:
    case MSR_K8_PSTATE_LIMIT:
    case MSR_K8_PSTATE_CTRL:
    case MSR_K8_PSTATE_STATUS:
    case MSR_K8_PSTATE0:
    case MSR_K8_PSTATE1:
    case MSR_K8_PSTATE2:
    case MSR_K8_PSTATE3:
    case MSR_K8_PSTATE4:
    case MSR_K8_PSTATE5:
    case MSR_K8_PSTATE6:
    case MSR_K8_PSTATE7:
        if ( boot_cpu_data.x86_vendor != X86_VENDOR_AMD )
            break;
        if ( unlikely(is_cpufreq_controller(currd)) )
            goto normal;
        *val = 0;
        return X86EMUL_OKAY;

    case MSR_IA32_UCODE_REV:
        BUILD_BUG_ON(MSR_IA32_UCODE_REV != MSR_AMD_PATCHLEVEL);
        if ( boot_cpu_data.x86_vendor == X86_VENDOR_INTEL )
        {
            if ( wrmsr_safe(MSR_IA32_UCODE_REV, 0) )
                break;
            /* As documented in the SDM: Do a CPUID 1 here */
            cpuid_eax(1);
        }
        goto normal;

    case MSR_IA32_MISC_ENABLE:
        if ( rdmsr_safe(reg, *val) )
            break;
        *val = guest_misc_enable(*val);
        return X86EMUL_OKAY;

    case MSR_AMD64_DR0_ADDRESS_MASK:
        if ( !boot_cpu_has(X86_FEATURE_DBEXT) )
            break;
        *val = curr->arch.pv_vcpu.dr_mask[0];
        return X86EMUL_OKAY;

    case MSR_AMD64_DR1_ADDRESS_MASK ... MSR_AMD64_DR3_ADDRESS_MASK:
        if ( !boot_cpu_has(X86_FEATURE_DBEXT) )
            break;
        *val = curr->arch.pv_vcpu.dr_mask[reg - MSR_AMD64_DR1_ADDRESS_MASK + 1];
        return X86EMUL_OKAY;

    case MSR_IA32_PERF_CAPABILITIES:
        /* No extra capabilities are supported. */
        *val = 0;
        return X86EMUL_OKAY;

    case MSR_PRED_CMD:
    case MSR_FLUSH_CMD:
        /* Write-only */
        break;

    case MSR_SPEC_CTRL:
        if ( !currd->arch.cpuid->feat.ibrsb )
            break;
        *val = curr->arch.spec_ctrl;
        return X86EMUL_OKAY;

    case MSR_INTEL_PLATFORM_INFO:
        if ( !boot_cpu_has(X86_FEATURE_MSR_PLATFORM_INFO) )
            break;
        *val = 0;
        if ( this_cpu(cpuid_faulting_enabled) )
            *val |= MSR_PLATFORM_INFO_CPUID_FAULTING;
        return X86EMUL_OKAY;

    case MSR_ARCH_CAPABILITIES:
        /* Not implemented yet. */
        break;

    case MSR_INTEL_MISC_FEATURES_ENABLES:
        if ( !boot_cpu_has(X86_FEATURE_MSR_MISC_FEATURES) )
            break;
        *val = 0;
        if ( curr->arch.cpuid_faulting )
            *val |= MSR_MISC_FEATURES_CPUID_FAULTING;
        return X86EMUL_OKAY;

    case MSR_P6_PERFCTR(0)...MSR_P6_PERFCTR(7):
    case MSR_P6_EVNTSEL(0)...MSR_P6_EVNTSEL(3):
    case MSR_CORE_PERF_FIXED_CTR0...MSR_CORE_PERF_FIXED_CTR2:
    case MSR_CORE_PERF_FIXED_CTR_CTRL...MSR_CORE_PERF_GLOBAL_OVF_CTRL:
        if ( boot_cpu_data.x86_vendor == X86_VENDOR_INTEL )
        {
            vpmu_msr = true;
            /* fall through */
    case MSR_AMD_FAM15H_EVNTSEL0...MSR_AMD_FAM15H_PERFCTR5:
    case MSR_K7_EVNTSEL0...MSR_K7_PERFCTR3:
            if ( vpmu_msr || (boot_cpu_data.x86_vendor == X86_VENDOR_AMD) )
            {
                if ( vpmu_do_rdmsr(reg, val) )
                    break;
                return X86EMUL_OKAY;
            }
        }
        /* fall through */
    default:
        if ( rdmsr_hypervisor_regs(reg, val) )
            return X86EMUL_OKAY;

        rc = vmce_rdmsr(reg, val);
        if ( rc < 0 )
            break;
        if ( rc )
            return X86EMUL_OKAY;
        /* fall through */
    normal:
        /* Everyone can read the MSR space. */
        /* gdprintk(XENLOG_WARNING, "Domain attempted RDMSR %08x\n", reg); */
        if ( rdmsr_safe(reg, *val) )
            break;
        return X86EMUL_OKAY;
    }

    return X86EMUL_UNHANDLEABLE;
}
