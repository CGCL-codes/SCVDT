static int update_domain_cpuid_info(struct domain *d,
                                    const struct xen_domctl_cpuid *ctl)
{
    struct cpuid_policy *p = d->arch.cpuid;
    const struct cpuid_leaf leaf = { ctl->eax, ctl->ebx, ctl->ecx, ctl->edx };
    int old_vendor = p->x86_vendor;
    unsigned int old_7d0 = p->feat.raw[0].d, old_e8b = p->extd.raw[8].b;
    bool call_policy_changed = false; /* Avoid for_each_vcpu() unnecessarily */

    /*
     * Skip update for leaves we don't care about, to avoid the overhead of
     * recalculate_cpuid_policy().
     */
    switch ( ctl->input[0] )
    {
    case 0x00000000 ... ARRAY_SIZE(p->basic.raw) - 1:
        if ( ctl->input[0] == 4 &&
             ctl->input[1] >= ARRAY_SIZE(p->cache.raw) )
            return 0;

        if ( ctl->input[0] == 7 &&
             ctl->input[1] >= ARRAY_SIZE(p->feat.raw) )
            return 0;

        if ( ctl->input[0] == 0xb &&
             ctl->input[1] >= ARRAY_SIZE(p->topo.raw) )
            return 0;

        BUILD_BUG_ON(ARRAY_SIZE(p->xstate.raw) < 2);
        if ( ctl->input[0] == XSTATE_CPUID &&
             ctl->input[1] != 1 ) /* Everything else automatically calculated. */
            return 0;
        break;

    case 0x40000000: case 0x40000100:
        /* Only care about the max_leaf limit. */

    case 0x80000000 ... 0x80000000 + ARRAY_SIZE(p->extd.raw) - 1:
        break;

    default:
        return 0;
    }

    /* Insert ctl data into cpuid_policy. */
    switch ( ctl->input[0] )
    {
    case 0x00000000 ... ARRAY_SIZE(p->basic.raw) - 1:
        switch ( ctl->input[0] )
        {
        case 4:
            p->cache.raw[ctl->input[1]] = leaf;
            break;

        case 7:
            p->feat.raw[ctl->input[1]] = leaf;
            break;

        case 0xb:
            p->topo.raw[ctl->input[1]] = leaf;
            break;

        case XSTATE_CPUID:
            p->xstate.raw[ctl->input[1]] = leaf;
            break;

        default:
            p->basic.raw[ctl->input[0]] = leaf;
            break;
        }
        break;

    case 0x40000000:
        p->hv_limit = ctl->eax;
        break;

    case 0x40000100:
        p->hv2_limit = ctl->eax;
        break;

    case 0x80000000 ... 0x80000000 + ARRAY_SIZE(p->extd.raw) - 1:
        p->extd.raw[ctl->input[0] - 0x80000000] = leaf;
        break;
    }

    recalculate_cpuid_policy(d);

    switch ( ctl->input[0] )
    {
    case 0:
        call_policy_changed = (p->x86_vendor != old_vendor);
        break;

    case 1:
        if ( is_pv_domain(d) && ((levelling_caps & LCAP_1cd) == LCAP_1cd) )
        {
            uint64_t mask = cpuidmask_defaults._1cd;
            uint32_t ecx = p->basic._1c;
            uint32_t edx = p->basic._1d;

            /*
             * Must expose hosts HTT and X2APIC value so a guest using native
             * CPUID can correctly interpret other leaves which cannot be
             * masked.
             */
            if ( cpu_has_x2apic )
                ecx |= cpufeat_mask(X86_FEATURE_X2APIC);
            if ( cpu_has_htt )
                edx |= cpufeat_mask(X86_FEATURE_HTT);

            switch ( boot_cpu_data.x86_vendor )
            {
            case X86_VENDOR_INTEL:
                /*
                 * Intel masking MSRs are documented as AND masks.
                 * Experimentally, they are applied after OSXSAVE and APIC
                 * are fast-forwarded from real hardware state.
                 */
                mask &= ((uint64_t)edx << 32) | ecx;

                if ( ecx & cpufeat_mask(X86_FEATURE_XSAVE) )
                    ecx = cpufeat_mask(X86_FEATURE_OSXSAVE);
                else
                    ecx = 0;
                edx = cpufeat_mask(X86_FEATURE_APIC);

                mask |= ((uint64_t)edx << 32) | ecx;
                break;

            case X86_VENDOR_AMD:
                mask &= ((uint64_t)ecx << 32) | edx;

                /*
                 * AMD masking MSRs are documented as overrides.
                 * Experimentally, fast-forwarding of the OSXSAVE and APIC
                 * bits from real hardware state only occurs if the MSR has
                 * the respective bits set.
                 */
                if ( ecx & cpufeat_mask(X86_FEATURE_XSAVE) )
                    ecx = cpufeat_mask(X86_FEATURE_OSXSAVE);
                else
                    ecx = 0;
                edx = cpufeat_mask(X86_FEATURE_APIC);

                mask |= ((uint64_t)ecx << 32) | edx;
                break;
            }

            d->arch.pv_domain.cpuidmasks->_1cd = mask;
        }
        break;

    case 6:
        if ( is_pv_domain(d) && ((levelling_caps & LCAP_6c) == LCAP_6c) )
        {
            uint64_t mask = cpuidmask_defaults._6c;

            if ( boot_cpu_data.x86_vendor == X86_VENDOR_AMD )
                mask &= (~0ULL << 32) | ctl->ecx;

            d->arch.pv_domain.cpuidmasks->_6c = mask;
        }
        break;

    case 7:
        if ( ctl->input[1] != 0 )
            break;

        if ( is_pv_domain(d) && ((levelling_caps & LCAP_7ab0) == LCAP_7ab0) )
        {
            uint64_t mask = cpuidmask_defaults._7ab0;
            uint32_t eax = ctl->eax;
            uint32_t ebx = p->feat._7b0;

            if ( boot_cpu_data.x86_vendor == X86_VENDOR_AMD )
                mask &= ((uint64_t)eax << 32) | ebx;

            d->arch.pv_domain.cpuidmasks->_7ab0 = mask;
        }

        /*
         * If the IBRS/IBPB policy has changed, we need to recalculate the MSR
         * interception bitmaps.
         */
        call_policy_changed = (is_hvm_domain(d) &&
                               ((old_7d0 ^ p->feat.raw[0].d) &
                                cpufeat_mask(X86_FEATURE_IBRSB)));
        break;

    case 0xa:
        if ( boot_cpu_data.x86_vendor != X86_VENDOR_INTEL )
            break;

        /* If PMU version is zero then the guest doesn't have VPMU */
        if ( p->basic.pmu_version == 0 )
        {
            struct vcpu *v;

            for_each_vcpu ( d, v )
                vpmu_destroy(v);
        }
        break;

    case 0xd:
        if ( ctl->input[1] != 1 )
            break;

        if ( is_pv_domain(d) && ((levelling_caps & LCAP_Da1) == LCAP_Da1) )
        {
            uint64_t mask = cpuidmask_defaults.Da1;
            uint32_t eax = p->xstate.Da1;

            if ( boot_cpu_data.x86_vendor == X86_VENDOR_INTEL )
                mask &= (~0ULL << 32) | eax;

            d->arch.pv_domain.cpuidmasks->Da1 = mask;
        }
        break;

    case 0x80000001:
        if ( is_pv_domain(d) && ((levelling_caps & LCAP_e1cd) == LCAP_e1cd) )
        {
            uint64_t mask = cpuidmask_defaults.e1cd;
            uint32_t ecx = p->extd.e1c;
            uint32_t edx = p->extd.e1d;

            /*
             * Must expose hosts CMP_LEGACY value so a guest using native
             * CPUID can correctly interpret other leaves which cannot be
             * masked.
             */
            if ( cpu_has_cmp_legacy )
                ecx |= cpufeat_mask(X86_FEATURE_CMP_LEGACY);

            /* If not emulating AMD, clear the duplicated features in e1d. */
            if ( p->x86_vendor != X86_VENDOR_AMD )
                edx &= ~CPUID_COMMON_1D_FEATURES;

            switch ( boot_cpu_data.x86_vendor )
            {
            case X86_VENDOR_INTEL:
                mask &= ((uint64_t)edx << 32) | ecx;
                break;

            case X86_VENDOR_AMD:
                mask &= ((uint64_t)ecx << 32) | edx;

                /*
                 * Fast-forward bits - Must be set in the masking MSR for
                 * fast-forwarding to occur in hardware.
                 */
                ecx = 0;
                edx = cpufeat_mask(X86_FEATURE_APIC);

                mask |= ((uint64_t)ecx << 32) | edx;
                break;
            }

            d->arch.pv_domain.cpuidmasks->e1cd = mask;
        }
        break;

    case 0x80000008:
        /*
         * If the IBPB policy has changed, we need to recalculate the MSR
         * interception bitmaps.
         */
        call_policy_changed = (is_hvm_domain(d) &&
                               ((old_e8b ^ p->extd.raw[8].b) &
                                cpufeat_mask(X86_FEATURE_IBPB)));
        break;
    }

    if ( call_policy_changed )
    {
        struct vcpu *v;

        for_each_vcpu( d, v )
            cpuid_policy_updated(v);
    }

    return 0;
}
