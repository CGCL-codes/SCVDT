int hvm_msr_write_intercept(unsigned int msr, uint64_t msr_content,
                            bool_t may_defer)
{
    struct vcpu *v = current;
    struct domain *d = v->domain;
    int ret = X86EMUL_OKAY;

    HVMTRACE_3D(MSR_WRITE, msr,
               (uint32_t)msr_content, (uint32_t)(msr_content >> 32));

    if ( may_defer && unlikely(monitored_msr(v->domain, msr)) )
    {
        ASSERT(v->arch.vm_event);

        /* The actual write will occur in hvm_do_resume() (if permitted). */
        v->arch.vm_event->write_data.do_write.msr = 1;
        v->arch.vm_event->write_data.msr = msr;
        v->arch.vm_event->write_data.value = msr_content;

        hvm_monitor_msr(msr, msr_content);
        return X86EMUL_OKAY;
    }

    switch ( msr )
    {
        unsigned int index;

    case MSR_EFER:
        if ( hvm_set_efer(msr_content) )
           return X86EMUL_EXCEPTION;
        break;

    case MSR_IA32_TSC:
        hvm_set_guest_tsc_msr(v, msr_content);
        break;

    case MSR_IA32_TSC_ADJUST:
        hvm_set_guest_tsc_adjust(v, msr_content);
        break;

    case MSR_TSC_AUX:
        v->arch.hvm_vcpu.msr_tsc_aux = (uint32_t)msr_content;
        if ( cpu_has_rdtscp
             && (v->domain->arch.tsc_mode != TSC_MODE_PVRDTSCP) )
            wrmsr_tsc_aux(msr_content);
        break;

    case MSR_IA32_APICBASE:
        if ( !vlapic_msr_set(vcpu_vlapic(v), msr_content) )
            goto gp_fault;
        break;

    case MSR_IA32_TSC_DEADLINE:
        vlapic_tdt_msr_set(vcpu_vlapic(v), msr_content);
        break;

    case MSR_IA32_APICBASE_MSR ... MSR_IA32_APICBASE_MSR + 0x3ff:
        if ( hvm_x2apic_msr_write(v, msr, msr_content) )
            goto gp_fault;
        break;

    case MSR_IA32_CR_PAT:
        if ( !hvm_set_guest_pat(v, msr_content) )
           goto gp_fault;
        break;

    case MSR_MTRRcap:
        goto gp_fault;

    case MSR_MTRRdefType:
        if ( !d->arch.cpuid->basic.mtrr )
            goto gp_fault;
        if ( !mtrr_def_type_msr_set(v->domain, &v->arch.hvm_vcpu.mtrr,
                                    msr_content) )
           goto gp_fault;
        break;
    case MSR_MTRRfix64K_00000:
        if ( !d->arch.cpuid->basic.mtrr )
            goto gp_fault;
        if ( !mtrr_fix_range_msr_set(v->domain, &v->arch.hvm_vcpu.mtrr, 0,
                                     msr_content) )
            goto gp_fault;
        break;
    case MSR_MTRRfix16K_80000:
    case MSR_MTRRfix16K_A0000:
        if ( !d->arch.cpuid->basic.mtrr )
            goto gp_fault;
        index = msr - MSR_MTRRfix16K_80000 + 1;
        if ( !mtrr_fix_range_msr_set(v->domain, &v->arch.hvm_vcpu.mtrr,
                                     index, msr_content) )
            goto gp_fault;
        break;
    case MSR_MTRRfix4K_C0000...MSR_MTRRfix4K_F8000:
        if ( !d->arch.cpuid->basic.mtrr )
            goto gp_fault;
        index = msr - MSR_MTRRfix4K_C0000 + 3;
        if ( !mtrr_fix_range_msr_set(v->domain, &v->arch.hvm_vcpu.mtrr,
                                     index, msr_content) )
            goto gp_fault;
        break;
    case MSR_IA32_MTRR_PHYSBASE(0)...MSR_IA32_MTRR_PHYSMASK(MTRR_VCNT-1):
        if ( !d->arch.cpuid->basic.mtrr )
            goto gp_fault;
        if ( !mtrr_var_range_msr_set(v->domain, &v->arch.hvm_vcpu.mtrr,
                                     msr, msr_content) )
            goto gp_fault;
        break;

    case MSR_AMD_PATCHLOADER:
        /*
         * See note on MSR_IA32_UCODE_WRITE below, which may or may not apply
         * to AMD CPUs as well (at least the architectural/CPUID part does).
         */
        if ( d->arch.cpuid->x86_vendor != X86_VENDOR_AMD )
            goto gp_fault;
        break;

    case MSR_IA32_UCODE_WRITE:
        /*
         * Some versions of Windows at least on certain hardware try to load
         * microcode before setting up an IDT. Therefore we must not inject #GP
         * for such attempts. Also the MSR is architectural and not qualified
         * by any CPUID bit.
         */
        if ( d->arch.cpuid->x86_vendor != X86_VENDOR_INTEL )
            goto gp_fault;
        break;

    case MSR_IA32_XSS:
        /* No XSS features currently supported for guests. */
        if ( !d->arch.cpuid->xstate.xsaves || msr_content != 0 )
            goto gp_fault;
        v->arch.hvm_vcpu.msr_xss = msr_content;
        break;

    case MSR_IA32_BNDCFGS:
        if ( !d->arch.cpuid->feat.mpx ||
             !hvm_set_guest_bndcfgs(v, msr_content) )
            goto gp_fault;
        break;

    case MSR_SPEC_CTRL:
        if ( !d->arch.cpuid->feat.ibrsb )
            goto gp_fault; /* MSR available? */

        /*
         * Note: SPEC_CTRL_STIBP is specified as safe to use (i.e. ignored)
         * when STIBP isn't enumerated in hardware.
         */

        if ( msr_content & ~(SPEC_CTRL_IBRS | SPEC_CTRL_STIBP |
                             (d->arch.cpuid->feat.ssbd ? SPEC_CTRL_SSBD : 0)) )
            goto gp_fault; /* Rsvd bit set? */

        v->arch.spec_ctrl = msr_content;
        break;

    case MSR_PRED_CMD:
        if ( !d->arch.cpuid->feat.ibrsb && !d->arch.cpuid->extd.ibpb )
            goto gp_fault; /* MSR available? */

        if ( msr_content & ~PRED_CMD_IBPB )
            goto gp_fault; /* Rsvd bit set? */

        wrmsrl(MSR_PRED_CMD, msr_content);
        break;

    case MSR_FLUSH_CMD:
        if ( !d->arch.cpuid->feat.l1d_flush )
            goto gp_fault; /* MSR available? */

        if ( msr_content & ~FLUSH_CMD_L1D )
            goto gp_fault; /* Rsvd bit set? */

        wrmsrl(MSR_FLUSH_CMD, msr_content);
        break;

    case MSR_ARCH_CAPABILITIES:
        /* Read-only */
        goto gp_fault;

    case MSR_AMD64_NB_CFG:
        /* ignore the write */
        break;

    default:
        if ( (ret = vmce_wrmsr(msr, msr_content)) < 0 )
            goto gp_fault;
        /* If ret == 0 then this is not an MCE MSR, see other MSRs. */
        ret = ((ret == 0)
               ? hvm_funcs.msr_write_intercept(msr, msr_content)
               : X86EMUL_OKAY);
        break;
    }

    return ret;

gp_fault:
    return X86EMUL_EXCEPTION;
}
