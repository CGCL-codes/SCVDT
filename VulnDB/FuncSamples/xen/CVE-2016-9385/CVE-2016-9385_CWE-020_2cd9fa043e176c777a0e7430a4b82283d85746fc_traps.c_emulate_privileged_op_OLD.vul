static int emulate_privileged_op(struct cpu_user_regs *regs)
{
    struct vcpu *v = current;
    struct domain *currd = v->domain;
    unsigned long *reg, eip = regs->eip;
    u8 opcode, modrm_reg = 0, modrm_rm = 0, rep_prefix = 0, lock = 0, rex = 0;
    enum { lm_seg_none, lm_seg_fs, lm_seg_gs } lm_ovr = lm_seg_none;
    int rc;
    unsigned int port, i, data_sel, ar, data, bpmatch = 0;
    unsigned int op_bytes, op_default, ad_bytes, ad_default, opsize_prefix= 0;
#define rd_ad(reg) (ad_bytes >= sizeof(regs->reg) \
                    ? regs->reg \
                    : ad_bytes == 4 \
                      ? (u32)regs->reg \
                      : (u16)regs->reg)
#define wr_ad(reg, val) (ad_bytes >= sizeof(regs->reg) \
                         ? regs->reg = (val) \
                         : ad_bytes == 4 \
                           ? (*(u32 *)&regs->reg = (val)) \
                           : (*(u16 *)&regs->reg = (val)))
    unsigned long code_base, code_limit;
    char *io_emul_stub = NULL;
    void (*io_emul)(struct cpu_user_regs *) __attribute__((__regparm__(1)));
    uint64_t val;
    bool_t vpmu_msr;

    if ( !read_descriptor(regs->cs, v, regs,
                          &code_base, &code_limit, &ar, 1) )
        goto fail;
    op_default = op_bytes = (ar & (_SEGMENT_L|_SEGMENT_DB)) ? 4 : 2;
    ad_default = ad_bytes = (ar & _SEGMENT_L) ? 8 : op_default;
    if ( !(ar & _SEGMENT_S) ||
         !(ar & _SEGMENT_P) ||
         !(ar & _SEGMENT_CODE) )
        goto fail;

    /* emulating only opcodes not allowing SS to be default */
    data_sel = read_sreg(ds);

    /* Legacy prefixes. */
    for ( i = 0; i < 8; i++, rex == opcode || (rex = 0) )
    {
        switch ( opcode = insn_fetch(u8, code_base, eip, code_limit) )
        {
        case 0x66: /* operand-size override */
            opsize_prefix = 1;
            op_bytes = op_default ^ 6; /* switch between 2/4 bytes */
            continue;
        case 0x67: /* address-size override */
            ad_bytes = ad_default != 4 ? 4 : 2; /* switch to 2/4 bytes */
            continue;
        case 0x2e: /* CS override */
            data_sel = regs->cs;
            continue;
        case 0x3e: /* DS override */
            data_sel = read_sreg(ds);
            continue;
        case 0x26: /* ES override */
            data_sel = read_sreg(es);
            continue;
        case 0x64: /* FS override */
            data_sel = read_sreg(fs);
            lm_ovr = lm_seg_fs;
            continue;
        case 0x65: /* GS override */
            data_sel = read_sreg(gs);
            lm_ovr = lm_seg_gs;
            continue;
        case 0x36: /* SS override */
            data_sel = regs->ss;
            continue;
        case 0xf0: /* LOCK */
            lock = 1;
            continue;
        case 0xf2: /* REPNE/REPNZ */
        case 0xf3: /* REP/REPE/REPZ */
            rep_prefix = 1;
            continue;
        default:
            if ( (ar & _SEGMENT_L) && (opcode & 0xf0) == 0x40 )
            {
                rex = opcode;
                continue;
            }
            break;
        }
        break;
    }

    /* REX prefix. */
    if ( rex & 8 ) /* REX.W */
        op_bytes = 4; /* emulate only opcodes not supporting 64-bit operands */
    modrm_reg = (rex & 4) << 1;  /* REX.R */
    /* REX.X does not need to be decoded. */
    modrm_rm  = (rex & 1) << 3;  /* REX.B */

    if ( opcode == 0x0f )
        goto twobyte_opcode;
    
    if ( lock )
        goto fail;

    /* Input/Output String instructions. */
    if ( (opcode >= 0x6c) && (opcode <= 0x6f) )
    {
        unsigned long data_base, data_limit;

        if ( rep_prefix && (rd_ad(ecx) == 0) )
            goto done;

        if ( !(opcode & 2) )
        {
            data_sel = read_sreg(es);
            lm_ovr = lm_seg_none;
        }

        if ( !(ar & _SEGMENT_L) )
        {
            if ( !read_descriptor(data_sel, v, regs,
                                  &data_base, &data_limit, &ar, 0) )
                goto fail;
            if ( !(ar & _SEGMENT_S) ||
                 !(ar & _SEGMENT_P) ||
                 (opcode & 2 ?
                  (ar & _SEGMENT_CODE) && !(ar & _SEGMENT_WR) :
                  (ar & _SEGMENT_CODE) || !(ar & _SEGMENT_WR)) )
                goto fail;
        }
        else
        {
            switch ( lm_ovr )
            {
            default:
                data_base = 0UL;
                break;
            case lm_seg_fs:
                data_base = rdfsbase();
                break;
            case lm_seg_gs:
                data_base = rdgsbase();
                break;
            }
            data_limit = ~0UL;
            ar = _SEGMENT_WR|_SEGMENT_S|_SEGMENT_DPL|_SEGMENT_P;
        }

        port = (u16)regs->edx;

    continue_io_string:
        switch ( opcode )
        {
        case 0x6c: /* INSB */
            op_bytes = 1;
        case 0x6d: /* INSW/INSL */
            if ( (data_limit < (op_bytes - 1)) ||
                 (rd_ad(edi) > (data_limit - (op_bytes - 1))) ||
                 !guest_io_okay(port, op_bytes, v, regs) )
                goto fail;
            data = guest_io_read(port, op_bytes, currd);
            if ( (rc = copy_to_user((void *)data_base + rd_ad(edi),
                                    &data, op_bytes)) != 0 )
            {
                propagate_page_fault(data_base + rd_ad(edi) + op_bytes - rc,
                                     PFEC_write_access);
                return EXCRET_fault_fixed;
            }
            wr_ad(edi, regs->edi + (int)((regs->eflags & X86_EFLAGS_DF)
                                         ? -op_bytes : op_bytes));
            break;

        case 0x6e: /* OUTSB */
            op_bytes = 1;
        case 0x6f: /* OUTSW/OUTSL */
            if ( (data_limit < (op_bytes - 1)) ||
                 (rd_ad(esi) > (data_limit - (op_bytes - 1))) ||
                  !guest_io_okay(port, op_bytes, v, regs) )
                goto fail;
            if ( (rc = copy_from_user(&data, (void *)data_base + rd_ad(esi),
                                      op_bytes)) != 0 )
            {
                propagate_page_fault(data_base + rd_ad(esi)
                                     + op_bytes - rc, 0);
                return EXCRET_fault_fixed;
            }
            guest_io_write(port, op_bytes, data, currd);
            wr_ad(esi, regs->esi + (int)((regs->eflags & X86_EFLAGS_DF)
                                         ? -op_bytes : op_bytes));
            break;
        }

        bpmatch = check_guest_io_breakpoint(v, port, op_bytes);

        if ( rep_prefix && (wr_ad(ecx, regs->ecx - 1) != 0) )
        {
            if ( !bpmatch && !hypercall_preempt_check() )
                goto continue_io_string;
            eip = regs->eip;
        }

        goto done;
    }

    /*
     * Very likely to be an I/O instruction (IN/OUT).
     * Build an stub to execute the instruction with full guest GPR
     * context. This is needed for some systems which (ab)use IN/OUT
     * to communicate with BIOS code in system-management mode.
     */
    io_emul_stub = map_domain_page(_mfn(this_cpu(stubs.mfn))) +
                   (this_cpu(stubs.addr) & ~PAGE_MASK) +
                   STUB_BUF_SIZE / 2;
    /* movq $host_to_guest_gpr_switch,%rcx */
    io_emul_stub[0] = 0x48;
    io_emul_stub[1] = 0xb9;
    *(void **)&io_emul_stub[2] = (void *)host_to_guest_gpr_switch;
    /* callq *%rcx */
    io_emul_stub[10] = 0xff;
    io_emul_stub[11] = 0xd1;
    /* data16 or nop */
    io_emul_stub[12] = (op_bytes != 2) ? 0x90 : 0x66;
    /* <io-access opcode> */
    io_emul_stub[13] = opcode;
    /* imm8 or nop */
    io_emul_stub[14] = 0x90;
    /* ret (jumps to guest_to_host_gpr_switch) */
    io_emul_stub[15] = 0xc3;
    BUILD_BUG_ON(STUB_BUF_SIZE / 2 < 16);

    /* Handy function-typed pointer to the stub. */
    io_emul = (void *)(this_cpu(stubs.addr) + STUB_BUF_SIZE / 2);

    if ( ioemul_handle_quirk )
        ioemul_handle_quirk(opcode, &io_emul_stub[12], regs);

    /* I/O Port and Interrupt Flag instructions. */
    switch ( opcode )
    {
    case 0xe4: /* IN imm8,%al */
        op_bytes = 1;
    case 0xe5: /* IN imm8,%eax */
        port = insn_fetch(u8, code_base, eip, code_limit);
        io_emul_stub[14] = port; /* imm8 */
    exec_in:
        if ( !guest_io_okay(port, op_bytes, v, regs) )
            goto fail;
        if ( admin_io_okay(port, op_bytes, currd) )
        {
            mark_regs_dirty(regs);
            io_emul(regs);            
        }
        else
        {
            if ( op_bytes == 4 )
                regs->eax = 0;
            else
                regs->eax &= ~((1 << (op_bytes * 8)) - 1);
            regs->eax |= guest_io_read(port, op_bytes, currd);
        }
        bpmatch = check_guest_io_breakpoint(v, port, op_bytes);
        goto done;

    case 0xec: /* IN %dx,%al */
        op_bytes = 1;
    case 0xed: /* IN %dx,%eax */
        port = (u16)regs->edx;
        goto exec_in;

    case 0xe6: /* OUT %al,imm8 */
        op_bytes = 1;
    case 0xe7: /* OUT %eax,imm8 */
        port = insn_fetch(u8, code_base, eip, code_limit);
        io_emul_stub[14] = port; /* imm8 */
    exec_out:
        if ( !guest_io_okay(port, op_bytes, v, regs) )
            goto fail;
        if ( admin_io_okay(port, op_bytes, currd) )
        {
            mark_regs_dirty(regs);
            io_emul(regs);            
            if ( (op_bytes == 1) && pv_post_outb_hook )
                pv_post_outb_hook(port, regs->eax);
        }
        else
        {
            guest_io_write(port, op_bytes, regs->eax, currd);
        }
        bpmatch = check_guest_io_breakpoint(v, port, op_bytes);
        goto done;

    case 0xee: /* OUT %al,%dx */
        op_bytes = 1;
    case 0xef: /* OUT %eax,%dx */
        port = (u16)regs->edx;
        goto exec_out;

    case 0xfa: /* CLI */
    case 0xfb: /* STI */
        if ( !iopl_ok(v, regs) )
            goto fail;
        /*
         * This is just too dangerous to allow, in my opinion. Consider if the
         * caller then tries to reenable interrupts using POPF: we can't trap
         * that and we'll end up with hard-to-debug lockups. Fast & loose will
         * do for us. :-)
         */
        /*v->vcpu_info->evtchn_upcall_mask = (opcode == 0xfa);*/
        goto done;
    }

    /* No decode of this single-byte opcode. */
    goto fail;

 twobyte_opcode:
    /*
     * All 2 and 3 byte opcodes, except RDTSC (0x31), RDTSCP (0x1,0xF9),
     * and CPUID (0xa2), are executable only from guest kernel mode 
     * (virtual ring 0).
     */
    opcode = insn_fetch(u8, code_base, eip, code_limit);
    if ( !guest_kernel_mode(v, regs) && 
        (opcode != 0x1) && (opcode != 0x31) && (opcode != 0xa2) )
        goto fail;

    if ( lock && (opcode & ~3) != 0x20 )
        goto fail;
    switch ( opcode )
    {
    case 0x1: /* RDTSCP and XSETBV */
        switch ( insn_fetch(u8, code_base, eip, code_limit) )
        {
        case 0xf9: /* RDTSCP */
            if ( (v->arch.pv_vcpu.ctrlreg[4] & X86_CR4_TSD) &&
                 !guest_kernel_mode(v, regs) )
                goto fail;
            pv_soft_rdtsc(v, regs, 1);
            break;
        case 0xd1: /* XSETBV */
        {
            u64 new_xfeature = (u32)regs->eax | ((u64)regs->edx << 32);

            if ( lock || rep_prefix || opsize_prefix
                 || !(v->arch.pv_vcpu.ctrlreg[4] & X86_CR4_OSXSAVE) )
            {
                do_guest_trap(TRAP_invalid_op, regs, 0);
                goto skip;
            }

            if ( !guest_kernel_mode(v, regs) )
                goto fail;

            if ( handle_xsetbv(regs->ecx, new_xfeature) )
                goto fail;

            break;
        }
        default:
            goto fail;
        }
        break;

    case 0x06: /* CLTS */
        (void)do_fpu_taskswitch(0);
        break;

    case 0x09: /* WBINVD */
        /* Ignore the instruction if unprivileged. */
        if ( !cache_flush_permitted(currd) )
            /* Non-physdev domain attempted WBINVD; ignore for now since
               newer linux uses this in some start-of-day timing loops */
            ;
        else
            wbinvd();
        break;

    case 0x20: /* MOV CR?,<reg> */
        opcode = insn_fetch(u8, code_base, eip, code_limit);
        if ( opcode < 0xc0 )
            goto fail;
        modrm_reg += ((opcode >> 3) & 7) + (lock << 3);
        modrm_rm  |= (opcode >> 0) & 7;
        reg = decode_register(modrm_rm, regs, 0);
        switch ( modrm_reg )
        {
        case 0: /* Read CR0 */
            *reg = (read_cr0() & ~X86_CR0_TS) |
                v->arch.pv_vcpu.ctrlreg[0];
            break;

        case 2: /* Read CR2 */
            *reg = v->arch.pv_vcpu.ctrlreg[2];
            break;
            
        case 3: /* Read CR3 */
        {
            unsigned long mfn;
            
            if ( !is_pv_32bit_domain(currd) )
            {
                mfn = pagetable_get_pfn(v->arch.guest_table);
                *reg = xen_pfn_to_cr3(mfn_to_gmfn(currd, mfn));
            }
            else
            {
                l4_pgentry_t *pl4e =
                    map_domain_page(_mfn(pagetable_get_pfn(v->arch.guest_table)));

                mfn = l4e_get_pfn(*pl4e);
                unmap_domain_page(pl4e);
                *reg = compat_pfn_to_cr3(mfn_to_gmfn(currd, mfn));
            }
            /* PTs should not be shared */
            BUG_ON(page_get_owner(mfn_to_page(mfn)) == dom_cow);
        }
        break;

        case 4: /* Read CR4 */
            *reg = v->arch.pv_vcpu.ctrlreg[4];
            break;

        default:
            goto fail;
        }
        break;

    case 0x21: /* MOV DR?,<reg> */ {
        unsigned long res;
        opcode = insn_fetch(u8, code_base, eip, code_limit);
        if ( opcode < 0xc0 )
            goto fail;
        modrm_reg += ((opcode >> 3) & 7) + (lock << 3);
        modrm_rm  |= (opcode >> 0) & 7;
        reg = decode_register(modrm_rm, regs, 0);
        if ( (res = do_get_debugreg(modrm_reg)) > (unsigned long)-256 )
            goto fail;
        *reg = res;
        break;
    }

    case 0x22: /* MOV <reg>,CR? */
        opcode = insn_fetch(u8, code_base, eip, code_limit);
        if ( opcode < 0xc0 )
            goto fail;
        modrm_reg += ((opcode >> 3) & 7) + (lock << 3);
        modrm_rm  |= (opcode >> 0) & 7;
        reg = decode_register(modrm_rm, regs, 0);
        switch ( modrm_reg )
        {
        case 0: /* Write CR0 */
            if ( (*reg ^ read_cr0()) & ~X86_CR0_TS )
            {
                gdprintk(XENLOG_WARNING,
                        "Attempt to change unmodifiable CR0 flags.\n");
                goto fail;
            }
            (void)do_fpu_taskswitch(!!(*reg & X86_CR0_TS));
            break;

        case 2: /* Write CR2 */
            v->arch.pv_vcpu.ctrlreg[2] = *reg;
            arch_set_cr2(v, *reg);
            break;

        case 3: {/* Write CR3 */
            unsigned long gfn;
            struct page_info *page;

            gfn = !is_pv_32bit_domain(currd)
                ? xen_cr3_to_pfn(*reg) : compat_cr3_to_pfn(*reg);
            page = get_page_from_gfn(currd, gfn, NULL, P2M_ALLOC);
            if ( page )
            {
                rc = new_guest_cr3(page_to_mfn(page));
                put_page(page);
            }
            else
                rc = -EINVAL;

            switch ( rc )
            {
            case 0:
                break;
            case -ERESTART: /* retry after preemption */
                goto skip;
            default:      /* not okay */
                goto fail;
            }
            break;
        }

        case 4: /* Write CR4 */
            v->arch.pv_vcpu.ctrlreg[4] = pv_guest_cr4_fixup(v, *reg);
            write_cr4(pv_guest_cr4_to_real_cr4(v));
            ctxt_switch_levelling(v);
            break;

        default:
            goto fail;
        }
        break;

    case 0x23: /* MOV <reg>,DR? */
        opcode = insn_fetch(u8, code_base, eip, code_limit);
        if ( opcode < 0xc0 )
            goto fail;
        modrm_reg += ((opcode >> 3) & 7) + (lock << 3);
        modrm_rm  |= (opcode >> 0) & 7;
        reg = decode_register(modrm_rm, regs, 0);
        if ( do_set_debugreg(modrm_reg, *reg) != 0 )
            goto fail;
        break;

    case 0x30: /* WRMSR */ {
        uint32_t eax = regs->eax;
        uint32_t edx = regs->edx;
        uint64_t msr_content = ((uint64_t)edx << 32) | eax;
        vpmu_msr = 0;
        switch ( regs->_ecx )
        {
        case MSR_FS_BASE:
            if ( is_pv_32bit_domain(currd) )
                goto fail;
            wrfsbase(msr_content);
            v->arch.pv_vcpu.fs_base = msr_content;
            break;
        case MSR_GS_BASE:
            if ( is_pv_32bit_domain(currd) )
                goto fail;
            wrgsbase(msr_content);
            v->arch.pv_vcpu.gs_base_kernel = msr_content;
            break;
        case MSR_SHADOW_GS_BASE:
            if ( is_pv_32bit_domain(currd) )
                goto fail;
            if ( wrmsr_safe(MSR_SHADOW_GS_BASE, msr_content) )
                goto fail;
            v->arch.pv_vcpu.gs_base_user = msr_content;
            break;
        case MSR_K7_FID_VID_STATUS:
        case MSR_K7_FID_VID_CTL:
        case MSR_K8_PSTATE_LIMIT:
        case MSR_K8_PSTATE_CTRL:
        case MSR_K8_PSTATE_STATUS:
        case MSR_K8_PSTATE0:
        case MSR_K8_PSTATE1:
        case MSR_K8_PSTATE2:
        case MSR_K8_PSTATE3:
        case MSR_K8_PSTATE4:
        case MSR_K8_PSTATE5:
        case MSR_K8_PSTATE6:
        case MSR_K8_PSTATE7:
        case MSR_K8_HWCR:
            if ( boot_cpu_data.x86_vendor != X86_VENDOR_AMD )
                goto fail;
            if ( !is_cpufreq_controller(currd) )
                break;
            if ( wrmsr_safe(regs->ecx, msr_content) != 0 )
                goto fail;
            break;
        case MSR_AMD64_NB_CFG:
            if ( boot_cpu_data.x86_vendor != X86_VENDOR_AMD ||
                 boot_cpu_data.x86 < 0x10 || boot_cpu_data.x86 > 0x17 )
                goto fail;
            if ( !is_hardware_domain(currd) || !is_pinned_vcpu(v) )
                break;
            if ( (rdmsr_safe(MSR_AMD64_NB_CFG, val) != 0) ||
                 (eax != (uint32_t)val) ||
                 ((edx ^ (val >> 32)) & ~(1 << (AMD64_NB_CFG_CF8_EXT_ENABLE_BIT - 32))) )
                goto invalid;
            if ( wrmsr_safe(MSR_AMD64_NB_CFG, msr_content) != 0 )
                goto fail;
            break;
        case MSR_FAM10H_MMIO_CONF_BASE:
            if ( boot_cpu_data.x86_vendor != X86_VENDOR_AMD ||
                 boot_cpu_data.x86 < 0x10 || boot_cpu_data.x86 > 0x17 )
                goto fail;
            if ( !is_hardware_domain(currd) || !is_pinned_vcpu(v) )
                break;
            if ( (rdmsr_safe(MSR_FAM10H_MMIO_CONF_BASE, val) != 0) )
                goto fail;
            if (
                 (pci_probe & PCI_PROBE_MASK) == PCI_PROBE_MMCONF ?
                 val != msr_content :
                 ((val ^ msr_content) &
                  ~( FAM10H_MMIO_CONF_ENABLE |
                    (FAM10H_MMIO_CONF_BUSRANGE_MASK <<
                     FAM10H_MMIO_CONF_BUSRANGE_SHIFT) |
                    ((u64)FAM10H_MMIO_CONF_BASE_MASK <<
                     FAM10H_MMIO_CONF_BASE_SHIFT))) )
                goto invalid;
            if ( wrmsr_safe(MSR_FAM10H_MMIO_CONF_BASE, msr_content) != 0 )
                goto fail;
            break;
        case MSR_IA32_UCODE_REV:
            if ( boot_cpu_data.x86_vendor != X86_VENDOR_INTEL )
                goto fail;
            if ( !is_hardware_domain(currd) || !is_pinned_vcpu(v) )
                break;
            if ( rdmsr_safe(regs->ecx, val) )
                goto fail;
            if ( msr_content )
                goto invalid;
            break;
        case MSR_IA32_MISC_ENABLE:
            if ( rdmsr_safe(regs->ecx, val) )
                goto fail;
            val = guest_misc_enable(val);
            if ( msr_content != val )
                goto invalid;
            break;
        case MSR_IA32_MPERF:
        case MSR_IA32_APERF:
            if (( boot_cpu_data.x86_vendor != X86_VENDOR_INTEL ) &&
                ( boot_cpu_data.x86_vendor != X86_VENDOR_AMD ) )
                goto fail;
            if ( !is_cpufreq_controller(currd) )
                break;
            if ( wrmsr_safe(regs->ecx, msr_content ) != 0 )
                goto fail;
            break;
        case MSR_IA32_PERF_CTL:
            if ( boot_cpu_data.x86_vendor != X86_VENDOR_INTEL )
                goto fail;
            if ( !is_cpufreq_controller(currd) )
                break;
            if ( wrmsr_safe(regs->ecx, msr_content) != 0 )
                goto fail;
            break;
        case MSR_IA32_THERM_CONTROL:
        case MSR_IA32_ENERGY_PERF_BIAS:
            if ( boot_cpu_data.x86_vendor != X86_VENDOR_INTEL )
                goto fail;
            if ( !is_hardware_domain(currd) || !is_pinned_vcpu(v) )
                break;
            if ( wrmsr_safe(regs->ecx, msr_content) != 0 )
                goto fail;
            break;

        case MSR_AMD64_DR0_ADDRESS_MASK:
            if ( !boot_cpu_has(X86_FEATURE_DBEXT) || (msr_content >> 32) )
                goto fail;
            v->arch.pv_vcpu.dr_mask[0] = msr_content;
            if ( v->arch.debugreg[7] & DR7_ACTIVE_MASK )
                wrmsrl(MSR_AMD64_DR0_ADDRESS_MASK, msr_content);
            break;
        case MSR_AMD64_DR1_ADDRESS_MASK ... MSR_AMD64_DR3_ADDRESS_MASK:
            if ( !boot_cpu_has(X86_FEATURE_DBEXT) || (msr_content >> 32) )
                goto fail;
            v->arch.pv_vcpu.dr_mask
                [regs->_ecx - MSR_AMD64_DR1_ADDRESS_MASK + 1] = msr_content;
            if ( v->arch.debugreg[7] & DR7_ACTIVE_MASK )
                wrmsrl(regs->_ecx, msr_content);
            break;

        case MSR_INTEL_PLATFORM_INFO:
            if ( boot_cpu_data.x86_vendor != X86_VENDOR_INTEL ||
                 msr_content ||
                 rdmsr_safe(MSR_INTEL_PLATFORM_INFO, msr_content) )
                goto fail;
            break;

        case MSR_P6_PERFCTR(0)...MSR_P6_PERFCTR(7):
        case MSR_P6_EVNTSEL(0)...MSR_P6_EVNTSEL(3):
        case MSR_CORE_PERF_FIXED_CTR0...MSR_CORE_PERF_FIXED_CTR2:
        case MSR_CORE_PERF_FIXED_CTR_CTRL...MSR_CORE_PERF_GLOBAL_OVF_CTRL:
            if ( boot_cpu_data.x86_vendor == X86_VENDOR_INTEL )
            {
                vpmu_msr = 1;
        case MSR_AMD_FAM15H_EVNTSEL0...MSR_AMD_FAM15H_PERFCTR5:
                if ( vpmu_msr || (boot_cpu_data.x86_vendor == X86_VENDOR_AMD) )
                {
                    if ( (vpmu_mode & XENPMU_MODE_ALL) &&
                         !is_hardware_domain(v->domain) )
                        break;

                    if ( vpmu_do_wrmsr(regs->ecx, msr_content, 0) )
                        goto fail;
                    break;
                }
            }
            /*FALLTHROUGH*/

        default:
            if ( wrmsr_hypervisor_regs(regs->ecx, msr_content) == 1 )
                break;

            rc = vmce_wrmsr(regs->ecx, msr_content);
            if ( rc < 0 )
                goto fail;
            if ( rc )
                break;

            if ( (rdmsr_safe(regs->ecx, val) != 0) || (msr_content != val) )
        invalid:
                gdprintk(XENLOG_WARNING, "Domain attempted WRMSR %p from "
                        "0x%016"PRIx64" to 0x%016"PRIx64".\n",
                        _p(regs->ecx), val, msr_content);
            break;
        }
        break;
    }

    case 0x31: /* RDTSC */
        if ( (v->arch.pv_vcpu.ctrlreg[4] & X86_CR4_TSD) &&
             !guest_kernel_mode(v, regs) )
            goto fail;
        if ( currd->arch.vtsc )
            pv_soft_rdtsc(v, regs, 0);
        else
        {
            val = rdtsc();
            goto rdmsr_writeback;
        }
        break;

    case 0x32: /* RDMSR */
        vpmu_msr = 0;
        switch ( regs->_ecx )
        {
        case MSR_FS_BASE:
            if ( is_pv_32bit_domain(currd) )
                goto fail;
            val = cpu_has_fsgsbase ? __rdfsbase() : v->arch.pv_vcpu.fs_base;
            goto rdmsr_writeback;
        case MSR_GS_BASE:
            if ( is_pv_32bit_domain(currd) )
                goto fail;
            val = cpu_has_fsgsbase ? __rdgsbase()
                                   : v->arch.pv_vcpu.gs_base_kernel;
            goto rdmsr_writeback;
        case MSR_SHADOW_GS_BASE:
            if ( is_pv_32bit_domain(currd) )
                goto fail;
            val = v->arch.pv_vcpu.gs_base_user;
            goto rdmsr_writeback;
        case MSR_K7_FID_VID_CTL:
        case MSR_K7_FID_VID_STATUS:
        case MSR_K8_PSTATE_LIMIT:
        case MSR_K8_PSTATE_CTRL:
        case MSR_K8_PSTATE_STATUS:
        case MSR_K8_PSTATE0:
        case MSR_K8_PSTATE1:
        case MSR_K8_PSTATE2:
        case MSR_K8_PSTATE3:
        case MSR_K8_PSTATE4:
        case MSR_K8_PSTATE5:
        case MSR_K8_PSTATE6:
        case MSR_K8_PSTATE7:
            if ( boot_cpu_data.x86_vendor != X86_VENDOR_AMD )
                goto fail;
            if ( !is_cpufreq_controller(currd) )
            {
                regs->eax = regs->edx = 0;
                break;
            }
            goto rdmsr_normal;
        case MSR_IA32_UCODE_REV:
            BUILD_BUG_ON(MSR_IA32_UCODE_REV != MSR_AMD_PATCHLEVEL);
            if ( boot_cpu_data.x86_vendor == X86_VENDOR_INTEL )
            {
                if ( wrmsr_safe(MSR_IA32_UCODE_REV, 0) )
                    goto fail;
                sync_core();
            }
            goto rdmsr_normal;
        case MSR_IA32_MISC_ENABLE:
            if ( rdmsr_safe(regs->ecx, val) )
                goto fail;
            val = guest_misc_enable(val);
            goto rdmsr_writeback;

        case MSR_AMD64_DR0_ADDRESS_MASK:
            if ( !boot_cpu_has(X86_FEATURE_DBEXT) )
                goto fail;
            regs->eax = v->arch.pv_vcpu.dr_mask[0];
            regs->edx = 0;
            break;
        case MSR_AMD64_DR1_ADDRESS_MASK ... MSR_AMD64_DR3_ADDRESS_MASK:
            if ( !boot_cpu_has(X86_FEATURE_DBEXT) )
                goto fail;
            regs->eax = v->arch.pv_vcpu.dr_mask
                            [regs->_ecx - MSR_AMD64_DR1_ADDRESS_MASK + 1];
            regs->edx = 0;
            break;
        case MSR_IA32_PERF_CAPABILITIES:
            /* No extra capabilities are supported */
            regs->eax = regs->edx = 0;
            break;

        case MSR_INTEL_PLATFORM_INFO:
            if ( boot_cpu_data.x86_vendor != X86_VENDOR_INTEL ||
                 rdmsr_safe(MSR_INTEL_PLATFORM_INFO, val) )
                goto fail;
            regs->eax = regs->edx = 0;
            break;

        case MSR_P6_PERFCTR(0)...MSR_P6_PERFCTR(7):
        case MSR_P6_EVNTSEL(0)...MSR_P6_EVNTSEL(3):
        case MSR_CORE_PERF_FIXED_CTR0...MSR_CORE_PERF_FIXED_CTR2:
        case MSR_CORE_PERF_FIXED_CTR_CTRL...MSR_CORE_PERF_GLOBAL_OVF_CTRL:
            if ( boot_cpu_data.x86_vendor == X86_VENDOR_INTEL )
            {
                vpmu_msr = 1;
        case MSR_AMD_FAM15H_EVNTSEL0...MSR_AMD_FAM15H_PERFCTR5:
                if ( vpmu_msr || (boot_cpu_data.x86_vendor == X86_VENDOR_AMD) )
                {

                    if ( (vpmu_mode & XENPMU_MODE_ALL) &&
                         !is_hardware_domain(v->domain) )
                    {
                        /* Don't leak PMU MSRs to unprivileged domains */
                        regs->eax = regs->edx = 0;
                        break;
                    }

                    if ( vpmu_do_rdmsr(regs->ecx, &val) )
                        goto fail;

                    regs->eax = (uint32_t)val;
                    regs->edx = (uint32_t)(val >> 32);
                    break;
                }
            }
            /*FALLTHROUGH*/

        default:
            if ( rdmsr_hypervisor_regs(regs->ecx, &val) )
                goto rdmsr_writeback;

            rc = vmce_rdmsr(regs->ecx, &val);
            if ( rc < 0 )
                goto fail;
            if ( rc )
                goto rdmsr_writeback;

        case MSR_EFER:
 rdmsr_normal:
            /* Everyone can read the MSR space. */
            /* gdprintk(XENLOG_WARNING,"Domain attempted RDMSR %p.\n",
                        _p(regs->ecx));*/
            if ( rdmsr_safe(regs->ecx, val) )
                goto fail;
 rdmsr_writeback:
            regs->eax = (uint32_t)val;
            regs->edx = (uint32_t)(val >> 32);
            break;
        }
        break;

    case 0xa2: /* CPUID */
        pv_cpuid(regs);
        break;

    default:
        goto fail;
    }

#undef wr_ad
#undef rd_ad

 done:
    instruction_done(regs, eip, bpmatch);
 skip:
    if ( io_emul_stub )
        unmap_domain_page(io_emul_stub);
    return EXCRET_fault_fixed;

 fail:
    if ( io_emul_stub )
        unmap_domain_page(io_emul_stub);
    return 0;
}
