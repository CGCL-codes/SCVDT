static int shmem_fault(struct vm_area_struct *vma, struct vm_fault *vmf)
{
	struct inode *inode = file_inode(vma->vm_file);
	int error;
	int ret = VM_FAULT_LOCKED;

	/*
	 * Trinity finds that probing a hole which tmpfs is punching can
	 * prevent the hole-punch from ever completing: which in turn
	 * locks writers out with its hold on i_mutex.  So refrain from
	 * faulting pages into the hole while it's being punched, and
	 * wait on i_mutex to be released if vmf->flags permits.
	 */
	if (unlikely(inode->i_private)) {
		struct shmem_falloc *shmem_falloc;

		spin_lock(&inode->i_lock);
		shmem_falloc = inode->i_private;
		if (!shmem_falloc ||
		    shmem_falloc->mode != FALLOC_FL_PUNCH_HOLE ||
		    vmf->pgoff < shmem_falloc->start ||
		    vmf->pgoff >= shmem_falloc->next)
			shmem_falloc = NULL;
		spin_unlock(&inode->i_lock);
		/*
		 * i_lock has protected us from taking shmem_falloc seriously
		 * once return from shmem_fallocate() went back up that stack.
		 * i_lock does not serialize with i_mutex at all, but it does
		 * not matter if sometimes we wait unnecessarily, or sometimes
		 * miss out on waiting: we just need to make those cases rare.
		 */
		if (shmem_falloc) {
			if ((vmf->flags & FAULT_FLAG_ALLOW_RETRY) &&
			   !(vmf->flags & FAULT_FLAG_RETRY_NOWAIT)) {
				up_read(&vma->vm_mm->mmap_sem);
				mutex_lock(&inode->i_mutex);
				mutex_unlock(&inode->i_mutex);
				return VM_FAULT_RETRY;
			}
			/* cond_resched? Leave that to GUP or return to user */
			return VM_FAULT_NOPAGE;
		}
	}

	error = shmem_getpage(inode, vmf->pgoff, &vmf->page, SGP_CACHE, &ret);
	if (error)
		return ((error == -ENOMEM) ? VM_FAULT_OOM : VM_FAULT_SIGBUS);

	if (ret & VM_FAULT_MAJOR) {
		count_vm_event(PGMAJFAULT);
		mem_cgroup_count_vm_event(vma->vm_mm, PGMAJFAULT);
	}
	return ret;
}
